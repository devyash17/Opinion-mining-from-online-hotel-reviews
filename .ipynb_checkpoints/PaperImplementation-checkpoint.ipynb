{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import * \n",
    "from tkinter.ttk import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tkinter import scrolledtext\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import random\n",
    "from collections import Counter\n",
    "from numpy import dot\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP.YO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\HP.YO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'small_dataset'\n",
    "ROOT_PATH = os.path.abspath(sys.path[0])\n",
    "DATA_PATH = os.path.join(ROOT_PATH,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(filename):\n",
    "    with open(filename) as f:\n",
    "        return [word for line in f for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = read_words(os.path.join(ROOT_PATH,'stopwords.txt'))\n",
    "english_words = read_words(os.path.join(ROOT_PATH,'english.txt'))\n",
    "adj = read_words('adjectives.txt')\n",
    "pwords = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\"]\n",
    "nwords = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_time(dataframe):\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        dataframe.at[i,'Date'] = datetime.datetime.strptime(dataframe['Date'][i],'%B %d, %Y')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dataframe(filename):\n",
    "    with open(filename) as data:\n",
    "        jarr = json.load(data)\n",
    "    dataframe = pd.DataFrame(jarr['Reviews'])\n",
    "    if(dataframe.shape[1] != 7):\n",
    "        return pd.DataFrame(columns = ['Key','Username','UserLocation','ReviewText','Date','ReviewRating','ReviewID','ReviewTitle'])\n",
    "    dataframe.columns = ['Username','UserLocation','ReviewText','Date','ReviewRating','ReviewID','ReviewTitle']\n",
    "    dataframe['Key'] = jarr['HotelInfo']['HotelID']\n",
    "    cols = dataframe.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]# Bring Key to 1st position\n",
    "    dataframe = dataframe[cols]\n",
    "    return dataframe\n",
    "#json_to_dataframe('./small_dataset/72572.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe():\n",
    "    main_df = pd.DataFrame(columns = ['Key','Username','UserLocation','ReviewText','Date','ReviewRating','ReviewID','ReviewTitle'])\n",
    "    for filename in os.listdir(DATA_PATH):\n",
    "        filepath = os.path.join(DATA_PATH,filename)\n",
    "        temp_df = json_to_dataframe(filepath)\n",
    "        main_df = pd.concat([main_df,temp_df])\n",
    "    main_df=main_df.reset_index()\n",
    "    main_df.columns = ['Index','Key','Username','UserLocation','ReviewText','Date','ReviewRating','ReviewID','ReviewTitle']\n",
    "    s = []\n",
    "    for index,row in main_df.iterrows():\n",
    "        sentences = [x for x in row['ReviewText'].split('.') if x != '' and x != ' ']\n",
    "        s.append(sentences)\n",
    "    main_df['Sentences']=s\n",
    "    return string_to_time(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_rating(dataframe):\n",
    "    av_rate = dict()\n",
    "    author_list = []\n",
    "    for index,row in dataframe.iterrows():\n",
    "        if row['Username'] not in author_list:\n",
    "            author_list.append(row['Username'])\n",
    "        if row['Key'] in av_rate.keys():#row key is hotel id\n",
    "            c = av_rate[row['Key']][0]*av_rate[row['Key']][1] #getting original rating sum ie mean*count =sum\n",
    "            c = c + float(row['ReviewRating']['Overall'])#adding another rating\n",
    "            c = c/(av_rate[row['Key']][1]+1)#getting new average\n",
    "            av_rate[row['Key']] = tuple([c,av_rate[row['Key']][1] + 1])#value=tuple of rating and no of ratings\n",
    "        else:\n",
    "            av_rate[row['Key']] = tuple([float(row['ReviewRating']['Overall']),1])\n",
    "    return (av_rate,author_list)#average rating of hotel along with total ratings and a list of authors\n",
    "\n",
    "def author_credibility(dataframe):#Definition 1\n",
    "    cred = dict()\n",
    "    credibility = dict()\n",
    "    (av_rate,author_list) = av_rating(dataframe)\n",
    "    for author in author_list:\n",
    "        cred[author] = tuple([0,0])#first number denotes the credibility of author and second number denotes total number of ratings given by author \n",
    "    for index,row in dataframe.iterrows():\n",
    "        c = cred[row['Username']][0] + abs((float(row['ReviewRating']['Overall']) - av_rate[row['Key']][0]))/5\n",
    "        #take credibility for existing an author and add to it (use av_rate with key and divide by 5)\n",
    "        cred[row['Username']]=tuple([c,cred[row['Username']][1]+1])#increasing the number of ratings given by author\n",
    "    for i in cred.keys():\n",
    "        credibility[i] = 1-(cred[i][0]/cred[i][1])#divide the summation\n",
    "    return credibility\n",
    "\n",
    "def review_recency(dataframe):#Definition 5\n",
    "    #returns recency list for all the hotels present in dataframe\n",
    "    #which contains the recency score according to the formula for each review\n",
    "    i = 0\n",
    "    query_time = datetime.datetime.now()\n",
    "    recency_list = np.array([])\n",
    "    for hotel_id in pd.unique(dataframe['Key']):\n",
    "        max_date = dataframe['Date'][i]#1st element is most recent\n",
    "        date_list = list()\n",
    "        while(dataframe['Key'][i] == hotel_id):\n",
    "            date_list.append(dataframe['Date'][i])\n",
    "            i = i+1\n",
    "            if(i == dataframe.shape[0]):\n",
    "                break\n",
    "        min_date = dataframe['Date'][i-1]#last element is least recent\n",
    "        dm = max_date - min_date #Time interval between first and final review for a hotel\n",
    "        date_list = [query_time-x for x in date_list]\n",
    "        date_list = np.array([np.exp(-x/dm) for x in date_list])\n",
    "        recency_list = np.append(recency_list,date_list)#appending date list for each hotel\n",
    "    return recency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_sentence_score(dataframe,weights):#Definition 6\n",
    "    listOfIndicators = read_words(os.path.join(ROOT_PATH,'indicator_phrases.txt'))#list of indicator words\n",
    "    #calculating css\n",
    "    calc_css = lambda css_scores: sum([a*b for a,b in zip(css_scores,weights)])#multiply corresponding elements and then compute sum\n",
    "    s = []#finallly it'll be list of list\n",
    "    for index,row in dataframe.iterrows():\n",
    "        sentences = row['Sentences']\n",
    "        max_words = max(len(sent.split()) for sent in sentences)\n",
    "        m = dict()\n",
    "        for k in sentences:\n",
    "            if k==row['ReviewTitle']:\n",
    "                m[k] = [1,0,0]#makes LOC=1\n",
    "            else:\n",
    "                m[k] = [0,0,0]\n",
    "            for i in listOfIndicators:\n",
    "                if i in k:\n",
    "                    m[k][1] = 1 # if it matches any indicator phrase then set it to 1\n",
    "                    break\n",
    "            m[k][2] = len(k.split())/max_words #ratio of number of words to the maximum number of words in a sentence\n",
    "        m[sentences[0]][0] = 1 #LOC=1 for the first sentence of the review\n",
    "        css = [tuple([m_val,calc_css(m[m_val])]) for m_val in m]#m_val is a sentencce\n",
    "        css = dict(css)\n",
    "        #print(css)\n",
    "        s.append([css])\n",
    "    return pd.DataFrame(s, columns = ['CSS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(sent1,sent2):\n",
    "    #sent1 is tagged sentence\n",
    "    words1 = Counter([x for x,_ in sent1])\n",
    "    words2 = Counter([x for x,_ in sent2])\n",
    "    intersect = set(words1.keys()) & set(words2.keys())\n",
    "    dot_product = sum([words1[x]*words2[x] for x in intersect])\n",
    "    prod = sum([words1[x]**2 for x in words1.keys()]) * sum([words2[x]**2 for x in words2.keys()])\n",
    "    if prod == 0:\n",
    "        return 0\n",
    "    return math.sqrt((dot_product**2)/prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdf(dataframe):\n",
    "    sent_single_list = []\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        sent_single_list.extend([[a,i,b] for a,b in enumerate(dataframe['Sentences'][i])])\n",
    "    sdataframe = pd.DataFrame(sent_single_list, columns = ['SentenceIndex','ReviewIndex','Sentence'])\n",
    "    return sdataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    for w in stop_words:\n",
    "        text = list(filter(lambda x: x != w, text))\n",
    "    return text\n",
    "\n",
    "def remove_space_and_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    stripped=text.translate(translator)\n",
    "    stripped = remove_stopwords(stripped.split())\n",
    "    return stripped\n",
    "#print(remove_space_and_punctuation('This, is a bad: thing.'))\n",
    "\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "is_adjective = lambda pos: pos[:2] == 'JJ' or pos[:2] == 'JJR' or pos[:2] == 'JJS'\n",
    "is_adverb = lambda pos: pos[:2] == 'RB' or pos[:2] == 'RBR' or pos[:2] == 'RBS'\n",
    "is_verb = lambda pos: pos[:2] == 'VB'\n",
    "\n",
    "def pos_tag(stripped):\n",
    "    return [(w, p) for (w, p) in nltk.pos_tag(stripped) if is_noun(p) or is_adjective(p) or is_adverb(p) or is_verb(p)] \n",
    "\n",
    "def clean_sentence(text):\n",
    "    stripped = remove_space_and_punctuation(text)\n",
    "    return pos_tag(stripped)\n",
    "\n",
    "def clean_sentences_list(sdataframe):\n",
    "    return [remove_space_and_punctuation(x) for x in sdataframe['Sentence']]\n",
    "\n",
    "def clean_sentences_dataframe(sdataframe):\n",
    "    sdataframe['Cleaned'] = [clean_sentence(x) for x in sdataframe['Sentence']]\n",
    "    sdataframe = sdataframe.reset_index()\n",
    "    sdataframe.drop('SentenceIndex', axis=1, inplace=True)\n",
    "    sdataframe.columns = ['SentenceIndex','ReviewIndex','Sentence','Cleaned']\n",
    "    return sdataframe\n",
    "\n",
    "def add_css(dataframe, sdataframe, cssdataframe):#Definition 7\n",
    "    #modifies the review sentence score to accomodate the review recency and author credibility\n",
    "    css_list = []\n",
    "    recency_list = review_recency(dataframe)#recency_list contains review recency score for all the hotels and for each review\n",
    "    cred_dict = author_credibility(dataframe)\n",
    "    cssdataframe=cssdataframe['CSS']\n",
    "    for _,x in sdataframe.iterrows():#ignoring index\n",
    "        css_list.append((cssdataframe[x['ReviewIndex']][x['Sentence']])*((recency_list[x['ReviewIndex']])+cred_dict[dataframe['Username'][x['ReviewIndex']]])/2)\n",
    "        #calculating sentence importance using author credibility and review recency.\n",
    "        #didn't have the upvotes for a review\n",
    "    sdataframe['CSS'] = css_list\n",
    "    return sdataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is using sentiwordnet\n",
    "# http://www.nltk.org/howto/sentiwordnet.html\n",
    "def get_senti_synsets(w,tag):\n",
    "    tag = get_pos_tag(tag)\n",
    "    if tag is None:\n",
    "        return None\n",
    "    else:\n",
    "        senti_synset = list(swn.senti_synsets(w,tag))#synonym set\n",
    "        if not senti_synset:\n",
    "            return None\n",
    "        return senti_synset[0]\n",
    "    \n",
    "def sentence_sentiment(sent1):\n",
    "    #passing as an argument already tagged sentences with pos tagging\n",
    "    syn1 = [get_senti_synsets(w,p) for (w,p) in sent1]\n",
    "    #print(syn1)\n",
    "    syn1 = [s for s in syn1 if s]\n",
    "    count = len(sent1)\n",
    "    \n",
    "    if count == 0:\n",
    "        return 0\n",
    "    \n",
    "    score = 0.0\n",
    "    for s in syn1:\n",
    "        if s.neg_score() > s.pos_score():\n",
    "            score = score - s.neg_score()\n",
    "        else:\n",
    "            score = score + s.pos_score()\n",
    "    print(\"senti:\")\n",
    "    print(score/count)\n",
    "    return score/count\n",
    "\n",
    "def senti_sim(sdf,ls1):\n",
    "    senti=[]\n",
    "    clean=sdf['Cleaned'].tolist()\n",
    "    for i in range(len(clean)):\n",
    "        senti.append(sentence_sentiment(sdf.iloc[i]['Cleaned']))\n",
    "    ls=[]\n",
    "    clean=sdf['Cleaned'].tolist()\n",
    "    for i in range(len(clean)):\n",
    "        ans=[]\n",
    "        for j in range(len(clean)):\n",
    "            ans.append(ls1[i][j]*abs(senti[i]+senti[j]))\n",
    "        ls.append(ans)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomise_medoids(k,n):\n",
    "    #print('random'+str(k))\n",
    "    return random.sample(range(n),k)\n",
    "\n",
    "def assign_point_to_cluster(sdataframe,sentence_index,cluster,ls):\n",
    "    #assigns a sentence on the basis of similarity to a cluster\n",
    "    maximum = 0\n",
    "    assigned_cluster = -1\n",
    "    for point in cluster:\n",
    "        sim = ls[sentence_index][point[0]]#getting similarity of sentence with the other cluster heads\n",
    "        if maximum<=sim:\n",
    "            maximum=sim\n",
    "            assigned_cluster = point\n",
    "    for point in cluster:\n",
    "        if point[0]==assigned_cluster[0]:\n",
    "            point[1].append(sentence_index)\n",
    "            break\n",
    "    return cluster\n",
    "\n",
    "def make_clusters(sdataframe,medoids,ls):\n",
    "    cluster = [[x,[]] for x in medoids]\n",
    "    for i,row in sdataframe.iterrows():\n",
    "        cluster = assign_point_to_cluster(sdataframe,i,cluster,ls)#assigns a sentence to suitable cluster\n",
    "    return cluster\n",
    "\n",
    "def find_similarity(sdf,similarity):\n",
    "    ls=[]\n",
    "    clean=sdf['Cleaned'].tolist()\n",
    "    for i in range(len(clean)):\n",
    "        ans=[]\n",
    "        for j in range(len(clean)):\n",
    "            ans.append(similarity(clean[i],clean[j]))\n",
    "        ls.append(ans)\n",
    "    return ls\n",
    "\n",
    "    \n",
    "def find_new_medoid_for_single_cluster(single_cluster,sdataframe,ls):\n",
    "    curr_sum=0\n",
    "    new_medoid = single_cluster[0]\n",
    "    for i in single_cluster[1]:#finding similarity with current cluster head\n",
    "        curr_sum = curr_sum + ls[single_cluster[0]][i]\n",
    "    for i in single_cluster[1]:#finding similarity with other points in cluster for possible new head of the cluster\n",
    "        nsum = 0\n",
    "        for j in single_cluster[1]:\n",
    "            nsum = nsum + ls[i][j]\n",
    "        if nsum > curr_sum:\n",
    "            curr_sum = nsum\n",
    "            new_medoid = i\n",
    "    return new_medoid\n",
    "\n",
    "def find_new_medoids(cluster,sdataframe,ls):\n",
    "    return [find_new_medoid_for_single_cluster(x,sdataframe,ls) for x in cluster]\n",
    "\n",
    "def kmedoids(k,sdataframe,similarity): \n",
    "    medoids = randomise_medoids(k,sdataframe.shape[0])#returns a list of k random integers\n",
    "    print(medoids)\n",
    "    ls=find_similarity(sdataframe,cosine_similarity)# a 2d list containing similarity of each sentence with other sentence\n",
    "    if(similarity!=cosine_similarity):\n",
    "        if similarity==sopmi_similarity:\n",
    "            ls=sopmi_similarity(sdataframe,ls)\n",
    "        else:\n",
    "            ls=senti_sim(sdataframe,ls)\n",
    "    for i in range(10):\n",
    "        cluster = make_clusters(sdataframe,medoids,ls)\n",
    "        medoids = find_new_medoids(cluster,sdataframe,ls)\n",
    "        #print(medoids)\n",
    "        if medoids == [a for a,b in cluster]:#convergence point\n",
    "            print(i,medoids)\n",
    "            break\n",
    "    return cluster\n",
    "\n",
    "def attach_importance(cluster,sdataframe):# cluster is of form [[x,[]],[y,[]],....]\n",
    "    for i,x in enumerate(cluster):\n",
    "        for j,y in enumerate(x[1]):\n",
    "            cluster[i][1][j] = tuple((y,sdataframe['CSS'][y]))\n",
    "        cluster[i][0] = [x[0],sum([a for _,a in x[1]])]#stores the overall scores of the cluster\n",
    "    \n",
    "    fin_cluster = []\n",
    "    for i,x in enumerate(cluster):\n",
    "        cluster[i][0].append(cluster[i][1])\n",
    "        fin_cluster.append(cluster[i][0])\n",
    "        fin_cluster[i][2].sort(key=lambda x: x[1], reverse=True)#sorts each cluster on the basis of scores\n",
    "    \n",
    "    fin_cluster.sort(key=lambda x: x[1], reverse=True)#sorts cluster on the basis of their overall scores    \n",
    "    return fin_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe()\n",
    "sdf1 = get_sdf(df)#get dataframe of sentences of review text for all reviews\n",
    "sdf2 = clean_sentences_dataframe(sdf1)#assigns pos tags in cleaned column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_imp = review_sentence_score(df,[0.3, 0.6, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = add_css(df, sdf2, sent_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwords = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\"]\n",
    "nwords = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\"]\n",
    "\n",
    "def create_vocabulary(sdataframe):#Of adjectives\n",
    "    vocab = []\n",
    "    for index,row in sdataframe.iterrows():\n",
    "        list1 = row[\"Cleaned\"]\n",
    "        for w in list1:\n",
    "            if w[1].startswith(\"J\") and w[0] not in vocab and w[0] in adj:\n",
    "                vocab.append(w[0])\n",
    "    vocab = {k: v for v, k in enumerate(vocab)}\n",
    "    return vocab\n",
    "\n",
    "def create_bow_of_adj(vocab,sdataframe):\n",
    "    #boolean vector\n",
    "    bag = [[0 for x in range(len(vocab))] for y in range(sdataframe.shape[0])]\n",
    "    for i,row in sdataframe.iterrows():\n",
    "        list1 = row[\"Cleaned\"]\n",
    "        for w in list1:\n",
    "            if w[1]==\"JJ\":\n",
    "                if w[0] in vocab.keys():\n",
    "                    bag[i][vocab[w[0]]] = 1\n",
    "    bag = np.array(bag).T.tolist()\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocabulary(sdf)\n",
    "bow = create_bow_of_adj(vocab,sdf)\n",
    "#each row corresponds to a word of vocab and column corresponds to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_pwords=1 #product of all p(t_l) where t_l is positive words\n",
    "for w in pwords:\n",
    "    if w in vocab.keys():\n",
    "        vec = bow[vocab[w]]\n",
    "        hits_pwords = hits_pwords*sum(vec)\n",
    "        # ultimately it will be product of all the observations in O(a_x)\n",
    "\n",
    "hits_nwords=1\n",
    "for w in nwords:#product of all p(t_l) where t_l is negative words\n",
    "    if w in vocab.keys():\n",
    "        vec = bow[vocab[w]]\n",
    "        hits_nwords = hits_nwords*sum(vec)\n",
    "\n",
    "def sopmi(word,hitsp,hitsn,bow,poswords,negwords):#returns O(a_x) sentiment strength score of an adjective\n",
    "    hitspwords=1\n",
    "    hitsnwords=1\n",
    "    if word in vocab.keys():\n",
    "        vecw = bow[vocab[word]]\n",
    "        for w in poswords:\n",
    "            if w in vocab.keys():\n",
    "                vecp = bow[vocab[w]]\n",
    "                hitspwords = hitspwords*(sum(x[0]*x[1] for x in zip(vecw,vecp))+1)\n",
    "        \n",
    "        for w in negwords:\n",
    "            if w in vocab.keys():\n",
    "                vecn = bow[vocab[w]]\n",
    "                hitsnwords = hitsnwords*(sum(x[0]*x[1] for x in zip(vecw,vecn))+1)\n",
    "        \n",
    "        return math.log((hitspwords*hitsn)/(hitsp*hitsnwords))/math.log(2)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def sent_polarity(sentence,hitsp,hitsn,bag,poswords,negwords):\n",
    "    #returns SP(S_j) by calculating O(S_j) \n",
    "    #here r is chosen to be 0.2\n",
    "    sums = 0.0\n",
    "    count = 0\n",
    "    for w in sentence:\n",
    "        k = sopmi(w,hitsp,hitsn,bag,poswords,negwords)\n",
    "        if k is not None:\n",
    "            count = count + 1\n",
    "            sums = sums + k\n",
    "    if count==0:\n",
    "        return 0\n",
    "    o_s = sums/count\n",
    "    if o_s > 0.2:\n",
    "        return 1\n",
    "    elif abs(o_s)<=0.2:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def sopmi_similarity(sdf,ls):#Sentence similarity using SOPMI and content similarity using cosine similarity\n",
    "    clean = sdf['Cleaned'].tolist()\n",
    "    \n",
    "    ls1=[] #Preprocessing the sentence polarity for each sentence.\n",
    "    for i in range(len(clean)):\n",
    "        ls1.append(sent_polarity(clean[i],hits_pwords,hits_nwords,bow,pwords,nwords))\n",
    "    \n",
    "    ans=[]\n",
    "    \n",
    "    for i in range(len (clean)):\n",
    "        temp=[]\n",
    "        for j in range(len(clean)):\n",
    "            s1 = ls1[i]\n",
    "            s2 = ls1[j]\n",
    "            if s1==s2:\n",
    "                temp.append(1*ls[i][j])\n",
    "            elif s1==0.5 or s2==0.5:\n",
    "                temp.append(0.5*ls[i][j])\n",
    "            else:\n",
    "                temp.append(0)\n",
    "        ans.append(temp)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_split = set(vocab.keys())\n",
    "cleaned_sentences = clean_sentences_list(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_to_dataframe(cluster,sdataframe):\n",
    "    cluster_list = []\n",
    "    # cluster: [centroid,overall score,[(sent index,sentence score),(),...]]\n",
    "    # x[0]=first parameter\n",
    "    for x in cluster[2]:\n",
    "        cluster_list.append(tuple((sdataframe.iloc[x[0]]['SentenceIndex'],sdataframe.iloc[x[0]]['Cleaned'])))\n",
    "    cluster_df = pd.DataFrame(cluster_list,columns = ['SentenceIndex','Cleaned'])\n",
    "    return cluster_df\n",
    "\n",
    "def postings_for_single_cluster(cdataframe,sdf):\n",
    "    dictionary = dict()\n",
    "    for index,rows in cdataframe.iterrows():\n",
    "        #takes all nouns and appends sentenceindex to a list in which they are present\n",
    "        for (word,p) in rows['Cleaned']:\n",
    "            if p.startswith('N'):\n",
    "                if word not in dictionary.keys():\n",
    "                    dictionary[word] =([],0)\n",
    "                dictionary[word][0].append(rows['SentenceIndex'])\n",
    "                \n",
    "    for key in dictionary.keys():\n",
    "        temp=set(dictionary[key][0])\n",
    "        dictionary[key] = (temp,len(temp))\n",
    "    \n",
    "    listd = []\n",
    "    \n",
    "    for k,v in dictionary.items():\n",
    "        listd.append([k,v])\n",
    "    listd.sort(key=lambda x: x[1][1], reverse=True)#sort according to number of occurrences in descending order\n",
    "    \n",
    "    for k in range(len(listd)):\n",
    "        ls = []\n",
    "        ls = sorted(listd[k][1][0],key=lambda x: sdf[\"CSS\"][x], reverse=True)#sorting index according to css score\n",
    "        listd[k][1] = (ls,listd[k][1][1])\n",
    "        #css is sentence importance score calculated using Definition #7\n",
    "        \n",
    "    return listd #returns a posting list with words as keys and a tuple containing sentence index and its frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postings_for_all_clusters(clusters,sdf):\n",
    "    lst = []\n",
    "    for cluster in clusters:\n",
    "        cdf = cluster_to_dataframe(cluster,sdf)#returns a dataframe with senetenceIndex and Cleaned as columns\n",
    "        words = postings_for_single_cluster(cdf,sdf)#a posting list [[word,([SentenceIndex],freq)],.....]\n",
    "        lst.append(words)\n",
    "    return lst#finally a list for all cluster with each element being a posting list in itself\n",
    "\n",
    "def top_k_sentences(imp,sdf):\n",
    "    sentences=\"\"\n",
    "    count=1\n",
    "    for i in range(len(imp)):#for taking a cluster\n",
    "        if imp[i]!=[]:\n",
    "            sentences+=str(count)+'. '+sdf[\"Sentence\"][imp[i][0][1][0][0]]+'\\n'\n",
    "            count+=1\n",
    "        else:\n",
    "            print('empty')\n",
    "            #imp[i][j][1][0] gives us the list of sentence index\n",
    "    return sentences #returns top k sentences from postings of all clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Steps to run GUI\n",
    "# Keep files in the small_dataset folder\n",
    "# Enter a number in the field.\n",
    "# Click enter button\n",
    "# Then select by which similarity measure you want to make clusters.\n",
    "# After a minute it will show the result\n",
    "\n",
    "\n",
    "window = Tk()\n",
    "window.title(\"Reviews Summarizer\")\n",
    " \n",
    "window.geometry('900x600')\n",
    "lbl = Label(window, text=\"How many clusters would you like to make?\")\n",
    "lbl.grid(column=0, row=0)\n",
    "\n",
    "inp = Entry(window,width=10)\n",
    "inp.grid(column=1, row=0)\n",
    "def clicked():\n",
    "    a=int(inp.get())\n",
    "    return a\n",
    "\n",
    "btn = Button(window, text=\"Enter\",command=clicked)\n",
    "btn.grid(column=2, row=0)\n",
    "txt = scrolledtext.ScrolledText(window,width=100,height=30,wrap='word')\n",
    "txt.grid(column=0,row=2,columnspan=3)\n",
    "\n",
    "def cos_tkinter():\n",
    "    txt.insert(INSERT,'Using COS Similarity\\n')\n",
    "    clst=kmedoids(clicked(),sdf,cosine_similarity)\n",
    "    clst=attach_importance(clst,sdf)\n",
    "    print(clst)\n",
    "    important = postings_for_all_clusters(clst,sdf)\n",
    "    important_sentences = top_k_sentences(important,sdf)\n",
    "    txt.insert(INSERT,str(important_sentences))\n",
    "    \n",
    "def sopmi_tkinter():\n",
    "    txt.insert(INSERT,'Using SOPMI Similarity\\n')\n",
    "    clst=kmedoids(clicked(),sdf,sopmi_similarity)\n",
    "    clst=attach_importance(clst,sdf)\n",
    "    important = postings_for_all_clusters(clst,sdf)\n",
    "    important_sentences = top_k_sentences(important,sdf)\n",
    "    txt.insert(INSERT,str(important_sentences))\n",
    "    \n",
    "def senti_tkinter():\n",
    "    txt.insert(INSERT,'Using SENTI Similarity\\n')\n",
    "    clst=kmedoids(clicked(),sdf,senti_sim)\n",
    "    clst=attach_importance(clst,sdf)\n",
    "    important = postings_for_all_clusters(clst,sdf)\n",
    "    important_sentences = top_k_sentences(important,sdf)\n",
    "    txt.insert(INSERT,str(important_sentences))\n",
    "\n",
    "rad1 = Radiobutton(window,text='cos', value=1,command=cos_tkinter)\n",
    "rad2 = Radiobutton(window,text='sopmi', value=2,command=sopmi_tkinter)\n",
    "rad3 = Radiobutton(window,text='wordsent', value=3,command=senti_tkinter)\n",
    "\n",
    "rad1.grid(column=0, row=1)\n",
    "rad2.grid(column=1, row=1)\n",
    "rad3.grid(column=2, row=1)\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_37_env",
   "language": "python",
   "name": "py_37_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
